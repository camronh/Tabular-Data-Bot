{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evals\n",
    "\n",
    "Need predict function and evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qU langsmith python-dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import evaluate\n",
    "from utils.langgraph import create_agent\n",
    "dataset_name = \"Movie Questions\"\n",
    "\n",
    "agent = create_agent()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict(inputs: dict):\n",
    "    return agent.invoke(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eval function\n",
    "from langsmith.schemas import Run, Example\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class CorrectnessScore(BaseModel):\n",
    "    reasoning: str = Field(\n",
    "        ..., description=\"Your reasoning for scoring the agent's answer as correct or incorrect\")\n",
    "    correct: bool = Field(..., description=\"If the agent's answer correct\")\n",
    "\n",
    "\n",
    "def correctness(run: Run, example: Example):\n",
    "    judge = ChatOpenAI(model=\"gpt-4o-mini\",\n",
    "                       temperature=0).with_structured_output(CorrectnessScore)\n",
    "    agent_answer = run.outputs[\"output\"]\n",
    "    correct_answer = example.outputs[\"output\"]\n",
    "    question = example.inputs[\"messages\"][-1][\"content\"]\n",
    "\n",
    "    system_prompt = f\"\"\"You are a lead QA analyst. Your job is to judge a movie data analyst's answer to users's questions. \\\n",
    "You will be provided with the correct answer in the form of a list of possible correct answers. You are judging what could be \\\n",
    "considered precision, as in as long as everything in the agent's answer is in the correct answer, it is considered correct.\n",
    "\n",
    "For example, if the agent answers with a list of movies and their details, and the correct answer only mentions the titles, the answer is still \\\n",
    "considered correct if the the titles match the correct answer.\n",
    "\n",
    "Another example is if the agent answers with a list of 3 movies and their details but the correct answer includes 10 movies. As long as the \\\n",
    "movies mentioned in the agent's response are included in the correct answer, the answer is still considered correct. \n",
    "\n",
    "If the agent did not answer with any movies, the answer is considered incorrect.\n",
    "\n",
    "\n",
    "<QUESTION>\n",
    "{question}\n",
    "</QUESTION>\n",
    "\n",
    "<AGENT_ANSWER>\n",
    "{agent_answer}\n",
    "</AGENT_ANSWER>\n",
    "\n",
    "<CORRECT_ANSWER>\n",
    "{correct_answer}\n",
    "</CORRECT_ANSWER>\n",
    "\"\"\"\n",
    "\n",
    "    result: CorrectnessScore = judge.invoke(system_prompt)\n",
    "    return {\"key\": \"correctness\", \"score\": 1 if result.correct else 0, \"comment\": result.reasoning}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get current git branch\n",
    "import subprocess\n",
    "\n",
    "def get_current_branch():\n",
    "    try:\n",
    "        result = subprocess.run(['git', 'rev-parse', '--abbrev-ref', 'HEAD'], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "        if result.returncode != 0:\n",
    "            return None\n",
    "        return result.stdout.decode('utf-8').strip()\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error getting current branch: {e}\")\n",
    "        return \"no-branch\"\n",
    "\n",
    "current_branch = get_current_branch()\n",
    "print(f\"Current branch: {current_branch}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "client = Client()\n",
    "\n",
    "evaluate(predict,\n",
    "         #  data=client.list_examples(dataset_name=dataset_name, splits=[\"sort\"]),\n",
    "         data=dataset_name,\n",
    "         evaluators=[correctness],\n",
    "         experiment_prefix=current_branch,\n",
    "        #  num_repetitions=2\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! [got my first failing evals](https://smith.langchain.com/o/d967989d-4221-53db-b0a5-665b504acba2/datasets/e1357a55-6663-4300-9695-34aad34bd544/compare?selectedSessions=6c31697f-8b1e-42e9-96c7-17ea0bbb01da)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
