{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evals\n",
    "\n",
    "Need predict function and evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU langsmith python-dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading df\n"
     ]
    }
   ],
   "source": [
    "from langsmith import evaluate\n",
    "from utils.langgraph import create_agent\n",
    "dataset_name = \"Movie Questions\"\n",
    "\n",
    "agent = create_agent()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict(inputs: dict):\n",
    "    return agent.invoke(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eval function\n",
    "from langsmith.schemas import Run, Example\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class CorrectnessScore(BaseModel):\n",
    "    reasoning: str = Field(\n",
    "        ..., description=\"Your reasoning for scoring the agent's answer as correct or incorrect\")\n",
    "    correct: bool = Field(..., description=\"If the agent's answer correct\")\n",
    "\n",
    "\n",
    "def correctness(run: Run, example: Example):\n",
    "    judge = ChatOpenAI(model=\"gpt-4o-mini\",\n",
    "                       temperature=0).with_structured_output(CorrectnessScore)\n",
    "    agent_answer = run.outputs[\"output\"]\n",
    "    correct_answer = example.outputs[\"output\"]\n",
    "    question = example.inputs[\"messages\"][-1][\"content\"]\n",
    "\n",
    "    system_prompt = f\"\"\"You are a lead QA analyst. Your job is to judge a movie data analyst's answer to users's questions. \\\n",
    "You will be provided with the correct answer in the form of a list of possible correct answers. You are judging what could be \\\n",
    "considered precision, as in as long as everything in the agent's answer is in the correct answer, it is considered correct.\n",
    "\n",
    "For example, if the agent answers with a list of movies and their details, and the correct answer only mentions the titles, the answer is still \\\n",
    "considered correct if the the titles match the correct answer.\n",
    "\n",
    "Another example is if the agent answers with a list of 3 movies and their details but the correct answer includes 10 movies. As long as the \\\n",
    "movies mentioned in the agent's response are included in the correct answer, the answer is still considered correct. \n",
    "\n",
    "If the agent did not answer with any movies, the answer is considered incorrect.\n",
    "\n",
    "\n",
    "<QUESTION>\n",
    "{question}\n",
    "</QUESTION>\n",
    "\n",
    "<AGENT_ANSWER>\n",
    "{agent_answer}\n",
    "</AGENT_ANSWER>\n",
    "\n",
    "<CORRECT_ANSWER>\n",
    "{correct_answer}\n",
    "</CORRECT_ANSWER>\n",
    "\"\"\"\n",
    "\n",
    "    result: CorrectnessScore = judge.invoke(system_prompt)\n",
    "    return {\"key\": \"correctness\", \"score\": 1 if result.correct else 0, \"comment\": result.reasoning}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current branch: filter\n"
     ]
    }
   ],
   "source": [
    "# Get current git branch\n",
    "import subprocess\n",
    "\n",
    "def get_current_branch():\n",
    "    try:\n",
    "        result = subprocess.run(['git', 'rev-parse', '--abbrev-ref', 'HEAD'], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "        if result.returncode != 0:\n",
    "            return None\n",
    "        return result.stdout.decode('utf-8').strip()\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error getting current branch: {e}\")\n",
    "        return \"no-branch\"\n",
    "\n",
    "current_branch = get_current_branch()\n",
    "print(f\"Current branch: {current_branch}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/camronhaider/Documents/Dev/tablular-data-bot/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'filter-4bf92908' at:\n",
      "https://smith.langchain.com/o/d967989d-4221-53db-b0a5-665b504acba2/datasets/e1357a55-6663-4300-9695-34aad34bd544/compare?selectedSessions=3bcb6747-79ed-4b41-879e-3b1f4da276ba\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got query embeddings\n",
      "Got query embeddings\n",
      "Got query embeddings\n",
      "Got query embeddings\n",
      "Got query embeddings\n",
      "Got query embeddings\n",
      "Got query embeddings\n",
      "Got query embeddings\n",
      "Got query embeddings\n",
      "Got query embeddings\n",
      "Got query embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:05,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got query embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:06,  1.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got query embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19it [00:19,  1.05s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ExperimentResults filter-4bf92908>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langsmith import Client\n",
    "client = Client()\n",
    "\n",
    "evaluate(predict,\n",
    "         #  data=client.list_examples(dataset_name=dataset_name, splits=[\"sort\"]),\n",
    "         data=dataset_name,\n",
    "         evaluators=[correctness],\n",
    "         experiment_prefix=current_branch,\n",
    "        #  num_repetitions=2\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! [got my first failing evals](https://smith.langchain.com/o/d967989d-4221-53db-b0a5-665b504acba2/datasets/e1357a55-6663-4300-9695-34aad34bd544/compare?selectedSessions=6c31697f-8b1e-42e9-96c7-17ea0bbb01da)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
